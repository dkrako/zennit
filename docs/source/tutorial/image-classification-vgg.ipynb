{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810f3c46",
   "metadata": {},
   "source": [
    "# Image Classification with VGG\n",
    "\n",
    "This tutorial will introduce the attribution of image classifiers using VGG11\n",
    "and ResNet18 on ImageNet. Feel free to replace VGG11 and ResNet18 with any\n",
    "other version of VGG or ResNet respectively.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "First, we install **Zennit**. This includes its dependencies `Pillow`,\n",
    "`torch` and `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zennit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dc4cd",
   "metadata": {},
   "source": [
    "Then, we import necessary modules, classes and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from torchvision.models import vgg11_bn, resnet18\n",
    "\n",
    "from zennit.attribution import Gradient\n",
    "from zennit.composites import EpsilonGammaBox, EpsilonPlusFlat\n",
    "from zennit.image import imgify, imsave\n",
    "from zennit.torchvision import VGGCanonizer, ResNetCanonizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e434e",
   "metadata": {},
   "source": [
    "We download an image of the [Dornbusch\n",
    "Lighthouse](https://en.wikipedia.org/wiki/Dornbusch_Lighthouse) from [Wikimedia\n",
    "Commons](https://commons.wikimedia.org/wiki/File:2006_09_06_180_Leuchtturm.jpg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.download_url_to_file(\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/2006_09_06_180_Leuchtturm.jpg/640px-2006_09_06_181_Leuchtturm.jpg',\n",
    "    'dornbusch-lighthouse.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798a0c1",
   "metadata": {},
   "source": [
    "We load and prepare the data. The image is resized such that the shorter side\n",
    "is 256 pixels in size, then center-cropped to `(224, 224)`, converted to a\n",
    "`torch.Tensor`, and then normalized according the channel-wise mean and\n",
    "standard deviation of the ImageNet dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base image transform\n",
    "transform_img = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "])\n",
    "# define the normalization transform\n",
    "transform_norm = Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "# define the full tensor transform\n",
    "transform = Compose([\n",
    "    transform_img,\n",
    "    ToTensor(),\n",
    "    transform_norm,\n",
    "])\n",
    "\n",
    "# load the image\n",
    "image = Image.open('dornbusch-lighthouse.jpg')\n",
    "\n",
    "# transform the PIL image and insert a batch-dimension\n",
    "data = transform(image)[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b4dd8",
   "metadata": {},
   "source": [
    "We can look at the original image and the cropped image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the original image\n",
    "display(image)\n",
    "# display the resized and cropped image\n",
    "display(transform_img(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45bd9b",
   "metadata": {},
   "source": [
    "## VGG11 - LRP with EpsilonGammaBox\n",
    "Then, we initialize the VGG16 model and load the hyperparameters. Set\n",
    "`pretrained=True` to use the pre-trained model instead of the random one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b50d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and set it to evaluation mode\n",
    "model = vgg11_bn(pretrained=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f677171",
   "metadata": {},
   "source": [
    "Compute the attribution using the ``EpsilonGammaBox`` **Composite**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17da931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the VGG-specific canonizer (alias for SequentialMergeBatchNorm, only\n",
    "# needed with batch-norm)\n",
    "canonizer = VGGCanonizer()\n",
    "\n",
    "# the EpsilonGammaBox composite needs the lowest and highest values, which are\n",
    "# here for ImageNet 0. and 1. with a different normalization for each channel\n",
    "low, high = transform_norm(torch.tensor([[[[[0.]]] * 3], [[[[1.]]] * 3]]))\n",
    "\n",
    "# create a composite, specifying arguments and the canonizers, if any\n",
    "composite = EpsilonGammaBox(low=low, high=high, canonizers=[canonizer])\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b5056",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9169bb3",
   "metadata": {},
   "source": [
    "Here, `imgify` produces a PIL-image, which can be saved with `.save()`.\n",
    "\n",
    "## More Visualization\n",
    "We can try out different color-maps by either using another built-in color map, or using the color-map specification language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Built-in color-map bwr')\n",
    "display(imgify(relevance, symmetric=True, cmap='bwr'))\n",
    "\n",
    "print('CMLS code for color-map from cyan to grey to purple')\n",
    "display(imgify(relevance, symmetric=True, cmap='0ff,444,f0f'))\n",
    "\n",
    "print('CMSL code for grey scale for negative and red for positive values')\n",
    "display(imgify(relevance, symmetric=True, cmap='fff,000,f00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf7029",
   "metadata": {},
   "source": [
    "To directly save the visualized attribution, we can use `imsave` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c2b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly save the visualized attribution\n",
    "imsave('attrib-1.png', relevance, symmetric=True, cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6dcd1",
   "metadata": {},
   "source": [
    "## ResNet18 - LRP with EpsilonPlusFlat\n",
    "Then, we initialize the ResNet18 model and load the hyperparameters. Set\n",
    "`pretrained=True` to use the pre-trained model instead of the random one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and set it to evaluation mode\n",
    "model = resnet18(pretrained=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dee524",
   "metadata": {},
   "source": [
    "Compute the attribution using the ``EpsilonPlusFlat`` **Composite**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cd10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ResNet-specific canonizer\n",
    "canonizer = ResNetCanonizer()\n",
    "\n",
    "# create a composite, specifying the canonizers, if any\n",
    "composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8f723",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1032120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
